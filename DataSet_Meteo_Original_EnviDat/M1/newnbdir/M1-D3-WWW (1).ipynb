{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook 3, Module 1, Data and Data Management, CAS Applied Data Science, 2019-08-23, S. Haug, University of Bern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data acquisition on the world wide web\n",
    "\n",
    "**Learning outcomes:**\n",
    "\n",
    "Participants will be able to collect data from www sources. Examples are provided and exercised.\n",
    "\n",
    "\n",
    "**Further sources**\n",
    "- Examples all over internet\n",
    "- A book: https://www.packtpub.com/big-data-and-business-intelligence/mastering-social-media-mining-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Analyse Aare with data from https://aareguru.existenz.ch/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data from website, bring it into a format which can be imported into a dataframe, plot the time series and the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import mathplot\n",
    "startlink = \"https://aareguru.existenz.ch/v2018/current\"\n",
    "f = urlopen(startlink)\n",
    "l = str(f.read())\n",
    "l=l.split('[')[3].split(']')\n",
    "l='['+l[0]+']'\n",
    "#print(l)\n",
    "df = pd.read_json(l)\n",
    "df.head()\n",
    "#df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df['temperature'].plot()\n",
    "plt.show()\n",
    "df['flow'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are flow and temperature correlated on this day? Let's look at the scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(df, alpha=0.2, figsize=(8, 8), diagonal='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a quite complicated correlation, if at all. Let's look at the summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possible further exercise or project for Module 1 and 2**\n",
    "\n",
    "Find some colleague who can the historical data (use the API) out of https://aareguru.existenz.ch/. Bring all data into one data frame. Look for correlations, averages (per month, per year ...). Combine the data with weather data, e.g. the wind on the Thun lake. For the Model 2 project, make a model predicting the Aare temperature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Get pictures (or files) from webpages\n",
    "\n",
    "Get a picture a show it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as image\n",
    "from IPython.display import display\n",
    "\n",
    "url = 'http://web.lhep.unibe.ch/shaug/lar/numuqe/evd.planeview.00001.000000001.png'\n",
    "img = image(url=url)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a file into the current directory (for further processing). In this exmple we downloud a file with tweets from the present US president. Then we load it into a dataframe for some analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import urllib\n",
    "fname=\"http://web.lhep.unibe.ch/shaug/tmp/t-tweets.txt\"\n",
    "urllib.request.urlretrieve(fname,'t-tweets.txt')\n",
    "df = pd.read_json('t-tweets.txt')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how often Obama was mentioned in these 245 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "txt = df['text']\n",
    "for i in range(0,len(txt)):\n",
    "    if 'Obama' in txt[i]: counter+=1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Yes, these are prefiltered tweets about Obama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Scrape Webpages (html scraping)\n",
    "\n",
    "There are almost two billion online websites. With python you can easily read and parse this data if you have the links. Here we get the links on the landing page of Science IT Suport, and if there are any, the mail addresses. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "\n",
    "#startlink = \"http://www.scits.unibe.ch\"\n",
    "startlink = \"http://www.scits.unibe.ch/about_us/contact/\"\n",
    "f = urlopen(startlink)\n",
    "myfile = f.read()\n",
    "lines = str(myfile).split(' ')\n",
    "links = []\n",
    "addresses = []\n",
    "for line in lines:\n",
    "    if 'http' in line:\n",
    "        links.append(line)\n",
    "    elif '@' in line:\n",
    "        #print(line)\n",
    "        addresses.append(line)\n",
    "df_links = pd.DataFrame(links,columns=['Links'])\n",
    "df_addrs = pd.DataFrame(addresses,columns=['Adresses'])\n",
    "df_addrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is not optimal as you have probably seen. Lets use regular expressions instead (from StackOverflow). Regular expressions are a bit geeky, but very powerful and great fun. If you don't wan't to learn them, you mostly find the expression you want by googling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # the regular expression module\n",
    "startlink = \"http://www.scits.unibe.ch/about_us/contact/\"\n",
    "f = urlopen(startlink)\n",
    "html = f.read()\n",
    "# Extract email addresses\n",
    "reobj = re.compile(r\"\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,6}\\b\", re.IGNORECASE)\n",
    "print(re.findall(reobj, html.decode('utf-8')))\n",
    "# Extract urls (links)\n",
    "urls = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', html.decode('utf-8'))\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible exercise\n",
    "\n",
    "Hier a nice little challenge for you. Use the code above (together with a for loop or two) to scrape all webpages of your employer company for public available email addresses and put them into a dataframe :) (of course you could scrape all internet now, but we don't go that far today)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write, copy and paste your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables from webpages\n",
    "\n",
    "If you or someone else pubslishes data in html tables, it can be collected with pandas quite easily, actually directly without using the urllib module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#link = \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population\"\n",
    "link = \"https://en.wikipedia.org/wiki/List_of_countries_by_electricity_consumption\"\n",
    "tables = pd.read_html(link)\n",
    "df = tables[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In which countries are the people consuming the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df = df.iloc[1:,1:2] \n",
    "s_df['consum'] = df.iloc[1:,7].astype('float')\n",
    "s_df.sort_values('consum', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4f7f38ad34ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'consum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hist'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 's_df' is not defined"
     ]
    }
   ],
   "source": [
    "s_df['consum'].plot(kind='hist',bins=50,range=(1000,50000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Run cron jobs (linux) / scheduled tasks (windows)\n",
    "\n",
    "If you need to collect data on a regular basis you typically run a so-called cron job on a Linux machine or a scheduled task on a Windows machine. For example, you can specify when and how often your Python data acquisition script should run. Maybe you would like to collect new Aare data every day. Of course, your computer has to be running and connected to internet if you collect data from internet.\n",
    "\n",
    "Here an example collecting Aare data. The python script aarecollect.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('aare-data-2019-08-23.json', <http.client.HTTPMessage at 0x104397828>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "import datetime\n",
    "link = \"https://aareguru.existenz.ch/v2018/current\"\n",
    "filename = 'aare-data-'+ str(datetime.date.today()) +'.json'\n",
    "urllib.request.urlretrieve(link,filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Linux you specify when to execute this file and how often in the so called crontab. On Windows by configuring a Scheduled Task. Google instructions if you need to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Access data bases via Python (mysql)\n",
    "\n",
    "You can access data bases with appropriate APIs from Python and also the Jupyter notebooks. One very popular type of database is mysql. Install the API module:\n",
    "\n",
    "pip install mysql-connector-python-rf\n",
    "\n",
    "If we have acces to a machine with a database, we can connect database and retrieve data to analyse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "#cnx = mysql.connector.connect(user='scott', password='password',\n",
    "#                              host='mycomputer.unibe.ch',\n",
    "#                              database='employees')\n",
    "#cnx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See examples on how to create tables, fill and query them in the manual: https://dev.mysql.com/doc/connector-python/en/connector-python-examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The follwing examples are not working, but indicates how one could data in different ways from social media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Google search\n",
    "\n",
    "There is are APIs for doing google searches from Python. Hier is one explained. \n",
    "\n",
    "https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search\n",
    "\n",
    "However, we didn't get it tp work right away. Maybe we used wrong key and id? \n",
    "\n",
    "If you need this, you will have to figure it out yourself.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_api_key = \"\" #\"Google API key\"\n",
    "my_cse_id = \"\"\n",
    "\n",
    "#def google_search(search_term, api_key, cse_id, **kwargs):\n",
    "    \n",
    "#service = build(\"customsearch\", \"v1\", developerKey=my_api_key)\n",
    "#res = service.cse().list(q='sigve haug', cx=my_cse_id, **kwargs).execute()\n",
    "#res = service.cse().list(q='sigve haug', cx=my_cse_id).execute()\n",
    "#    return res['items']\n",
    "\n",
    "#results = google_search('stackoverflow site:en.wikipedia.org', my_api_key, my_cse_id, num=10)\n",
    "#for result in results:\n",
    "#    pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Twitter\n",
    "\n",
    "**Note:** This example we couldn't execute because Twitter didn't yet approve the application request.\n",
    "\n",
    "Twitter generates about 500M tweets per day. Thus, data mining on twitter can be interesting.\n",
    "\n",
    "Note: there are rate limits in the use of the Twitter API, as well as limitations in case you want to provide a downloadable data-set, see:\n",
    "\n",
    "https://dev.twitter.com/overview/terms/agreement-and-policy\n",
    "\n",
    "https://dev.twitter.com/rest/public/rate-limiting\n",
    "\n",
    "Tweepy is one python module with clients for thwe Twitter API. Install it from your terminal/console:\n",
    "\n",
    "- pip install tweepy==3.3.0\n",
    "\n",
    "Now we need some login information from Twitter. So we must register an application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point your browser to http://apps.twitter.com, log-in to Twitter (if you’re not already logged in) and register a new application. You can now choose a name and a description for your app (for example “Mining Demo” or similar). You will receive a consumer key and a consumer secret: these are application settings that should always be kept private. From the configuration page of your app, you can also require an access token and an access token secret. Similarly to the consumer keys, these strings must also be kept private: they provide the application access to Twitter on behalf of your account. The default permissions are read-only, which is all we need in our case, but if you decide to change your permission to provide writing features in your app, you must negotiate a new access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    " \n",
    "consumer_key = 'YOUR-CONSUMER-KEY'\n",
    "consumer_secret = 'YOUR-CONSUMER-SECRET'\n",
    "access_token = 'YOUR-ACCESS-TOKEN'\n",
    "access_secret = 'YOUR-ACCESS-SECRET'\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Instagram\n",
    "\n",
    "Largest photo sharing social media platform with 500 million monthly active users, and 95 million pictures and videos uploaded on Instagram daily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an Instagram API. From the command line :\n",
    "\n",
    "`python -m pip install -e git+https://github.com/LevPasha/Instagram-API-python.git#egg=InstagramAPI`\n",
    "\n",
    "And we need ffmpeg. We can get it like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "imageio.plugins.ffmpeg.download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging in to Instagram Using the API. The login process takes some seconds and then returns a success message if successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from InstagramAPI import InstagramAPI\n",
    "username=\"sigvehaugbern\" # sigvehaugbern\n",
    "InstagramAPI = InstagramAPI(username, <password>) # \n",
    "InstagramAPI.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InstagramAPI.getProfileData()\n",
    "result = InstagramAPI.LastJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (result['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InstagramAPI.timelineFeed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = InstagramAPI.LastJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the pictures from your account in the Jupyter notebook\n",
    "\n",
    "In the above json dictionary we have the link to the images. We can display them like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as image\n",
    "from IPython.display import display\n",
    "\n",
    "url = 'https://instagram.fqls1-1.fna.fbcdn.net/vp/a881379f7a5d9ada39e60dd5791344a7/5C0DA510/t51.2885-15/sh0.08/e35/s750x750/39026032_683146402062694_5636277589606662144_n.jpg?ig_cache_key=MTg0NzI1ODEwNjE1ODA2NDU1OA%3D%3D.2'\n",
    "\n",
    "img = image(url=url)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kdnuggets.com/2017/08/instagram-python-data-analysis.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Module 3 you may do as project deep learning on pictures. Can you teach your deep neural network to distinguish a human from a pig? Or a male human face from a female human face? Instagram could be a source of pictures, e.g. by hash tags. If you do supervised learning, the annotation of the pictures is the larger job.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last word\n",
    "\n",
    "The power is yours. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
